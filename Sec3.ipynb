{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4e5d49",
   "metadata": {},
   "source": [
    "# Neural Logic Gates Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4db6bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddecda",
   "metadata": {},
   "source": [
    "## Step Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4920a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x):\n",
    "    \"\"\"Step activation function\"\"\"\n",
    "    return 1 if x >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0b7b1",
   "metadata": {},
   "source": [
    "## Basic Neuron Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "045e3859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(x, w, b):\n",
    "    \"\"\"Simple neuron with step activation\"\"\"\n",
    "    return step(x @ w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d0d96",
   "metadata": {},
   "source": [
    "## Basic Logic Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90a65136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_p(x):\n",
    "    \"\"\"AND gate implementation\"\"\"\n",
    "    n = neuron(x, np.array([1, 1]), -2)\n",
    "    return n\n",
    "\n",
    "def or_p(x):\n",
    "    \"\"\"OR gate implementation\"\"\"\n",
    "    n = neuron(x, np.array([1, 1]), -1)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5f23b",
   "metadata": {},
   "source": [
    "## XOR Gate Implementations\n",
    "\n",
    "Two different implementations of XOR gate using neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a222eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xor1_p(x):\n",
    "    \"\"\"XOR gate implementation - Method 1\"\"\"\n",
    "    n1 = neuron(x, np.array([1, -1]), -1)\n",
    "    n2 = neuron(x, np.array([-1, 1]), -1)\n",
    "    n3 = neuron(np.array([n1, n2]), np.array([1, 1]), -1)\n",
    "    return n3\n",
    "\n",
    "def xor2_p(x):\n",
    "    \"\"\"XOR gate implementation - Method 2\"\"\"\n",
    "    n1 = or_p(x)\n",
    "    n2 = neuron(x, np.array([-1, -1]), 1)\n",
    "    n3 = and_p(np.array([n1, n2]))\n",
    "    return n3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91717a",
   "metadata": {},
   "source": [
    "## Complex Boolean Function\n",
    "\n",
    "Implementation of a more complex boolean function using multiple neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbac6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_mlp(x):\n",
    "    \"\"\"Multi-layer perceptron for complex boolean function\"\"\"\n",
    "    n1 = neuron(x[[0, 1, 3]], np.array([1, -1, 1]), -2)\n",
    "    n2 = neuron(x[[0, 2]], np.array([1, -1]), -1)\n",
    "    n3 = neuron(x[[1, 2]], np.array([1, 1]), -2)\n",
    "    n4 = neuron(x[[1, 3]], np.array([1, 1]), -2)\n",
    "    n5 = or_p(np.array([n1, n2]))\n",
    "    n6 = neuron(np.array([n3, n4]), np.array([1, -1]), 0)\n",
    "    n7 = and_p(np.array([n5, n6]))\n",
    "    return n7\n",
    "\n",
    "def bf_exp(x):\n",
    "    \"\"\"Explicit boolean function implementation\"\"\"\n",
    "    return ((x[0] and not x[1] and x[3]) or (x[0] and not x[2])) and ((x[1] and x[2]) or not (x[1] and x[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed4dfa6",
   "metadata": {},
   "source": [
    "## Test All Possible Input Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3bd4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A, X, Y, Z)\tMLP Output\tExplicit Output\n",
      "---------------------------------------------\n",
      "[0 0 0 0]\t0\t\tFalse\n",
      "[0 0 0 1]\t0\t\tFalse\n",
      "[0 0 1 0]\t0\t\tFalse\n",
      "[0 0 1 1]\t0\t\tFalse\n",
      "[0 1 0 0]\t0\t\tFalse\n",
      "[0 1 0 1]\t0\t\tFalse\n",
      "[0 1 1 0]\t0\t\tFalse\n",
      "[0 1 1 1]\t0\t\tFalse\n",
      "[1 0 0 0]\t1\t\tTrue\n",
      "[1 0 0 1]\t1\t\tTrue\n",
      "[1 0 1 0]\t0\t\tFalse\n",
      "[1 0 1 1]\t1\t\tTrue\n",
      "[1 1 0 0]\t1\t\tTrue\n",
      "[1 1 0 1]\t0\t\tFalse\n",
      "[1 1 1 0]\t0\t\tFalse\n",
      "[1 1 1 1]\t0\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# Generate all possible input combinations for 4 binary variables\n",
    "x = [\n",
    "    [a, x, y, z] for a in range(2) for x in range(2) for y in range(2) for z in range(2)\n",
    "]\n",
    "x = np.array(x)\n",
    "\n",
    "print(\"(A, X, Y, Z)\\tMLP Output\\tExplicit Output\")\n",
    "print(\"-\" * 45)\n",
    "for e in x:\n",
    "    mlp_result = bf_mlp(e)\n",
    "    exp_result = bool(bf_exp(e))\n",
    "    print(f\"{e}\\t{mlp_result}\\t\\t{exp_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00475824",
   "metadata": {},
   "source": [
    "## Test Basic Logic Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc6c979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Basic Logic Gates:\n",
      "AND Gate:\n",
      "AND(0, 0) = 0\n",
      "AND(0, 1) = 0\n",
      "AND(1, 0) = 0\n",
      "AND(1, 1) = 1\n",
      "\n",
      "OR Gate:\n",
      "OR(0, 0) = 0\n",
      "OR(0, 1) = 1\n",
      "OR(1, 0) = 1\n",
      "OR(1, 1) = 1\n",
      "\n",
      "XOR Gate (Method 1):\n",
      "XOR(0, 0) = 0\n",
      "XOR(0, 1) = 1\n",
      "XOR(1, 0) = 1\n",
      "XOR(1, 1) = 0\n",
      "\n",
      "XOR Gate (Method 2):\n",
      "XOR(0, 0) = 0\n",
      "XOR(0, 1) = 1\n",
      "XOR(1, 0) = 1\n",
      "XOR(1, 1) = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting Basic Logic Gates:\")\n",
    "print(\"AND Gate:\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        inputs = np.array([i, j])\n",
    "        print(f\"AND({i}, {j}) = {and_p(inputs)}\")\n",
    "\n",
    "print(\"\\nOR Gate:\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        inputs = np.array([i, j])\n",
    "        print(f\"OR({i}, {j}) = {or_p(inputs)}\")\n",
    "\n",
    "print(\"\\nXOR Gate (Method 1):\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        inputs = np.array([i, j])\n",
    "        print(f\"XOR({i}, {j}) = {xor1_p(inputs)}\")\n",
    "\n",
    "print(\"\\nXOR Gate (Method 2):\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        inputs = np.array([i, j])\n",
    "        print(f\"XOR({i}, {j}) = {xor2_p(inputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007aa67",
   "metadata": {},
   "source": [
    "### Implementing a **generalized AND perceptron** capable of handling both positive and negated input literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e514a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_and(x):\n",
    "    # x: dict of {var_name: (value, is_positive)}\n",
    "    values = []\n",
    "    weights = []\n",
    "    L = 0\n",
    "\n",
    "    for _, (val, is_positive) in x.items():\n",
    "        w = 1 if is_positive else -1\n",
    "        weights.append(w)\n",
    "        values.append(val)  # this should be 0 or 1\n",
    "        if is_positive:\n",
    "            L += 1\n",
    "\n",
    "    x_vec = np.array(values)\n",
    "    w_vec = np.array(weights)\n",
    "\n",
    "    # bias must be based on number of positive literals\n",
    "    bias = -L\n",
    "\n",
    "    return neuron(x_vec, w_vec, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3d865a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x y z | perceptron expected\n",
      "--------------------------\n",
      "0 0 0 |    0       0\n",
      "0 0 1 |    0       0\n",
      "0 1 0 |    0       0\n",
      "0 1 1 |    0       0\n",
      "1 0 0 |    0       0\n",
      "1 0 1 |    0       0\n",
      "1 1 0 |    1       1\n",
      "1 1 1 |    0       0\n"
     ]
    }
   ],
   "source": [
    "# Variable order we will use: [\"x\", \"y\", \"z\"]\n",
    "\n",
    "print(\"x y z | perceptron expected\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "for x_val, y_val, z_val in product([0, 1], repeat=3):\n",
    "    # prepare dict: (value, is_positive)\n",
    "    # x and y are positive literals, z is negated\n",
    "    inputs = {\n",
    "        \"x\": (x_val, True),\n",
    "        \"y\": (y_val, True),\n",
    "        \"z\": (z_val, False),\n",
    "    }\n",
    "\n",
    "    percep_out = general_and(inputs)\n",
    "    expected = int((x_val) and (y_val) and not (z_val))  # expected output\n",
    "\n",
    "    print(f\"{x_val} {y_val} {z_val} |    {percep_out}       {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6a622",
   "metadata": {},
   "source": [
    "### Implementing a **generalized OR perceptron** capable of handling both positive and negated input literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e14983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_or(x):\n",
    "    # x: dict of {var_name: (value, is_positive)}\n",
    "    values = []\n",
    "    weights = []\n",
    "    L = 0  # Positive literals count\n",
    "\n",
    "    for _, (val, is_positive) in x.items():\n",
    "        w = 1 if is_positive else -1\n",
    "        weights.append(w)\n",
    "        values.append(val)  # this should be 0 or 1\n",
    "        if is_positive:\n",
    "            L += 1\n",
    "\n",
    "    x_vec = np.array(values)\n",
    "    w_vec = np.array(weights)\n",
    "\n",
    "    # bias must be based on number of positive literals - Total inputs + 1\n",
    "    bias = -(L - x_vec.size + 1)\n",
    "\n",
    "    return neuron(x_vec, w_vec, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a617a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x y z | perceptron expected\n",
      "--------------------------\n",
      "0 0 0 |    1       1\n",
      "0 0 1 |    0       0\n",
      "0 1 0 |    1       1\n",
      "0 1 1 |    1       1\n",
      "1 0 0 |    1       1\n",
      "1 0 1 |    1       1\n",
      "1 1 0 |    1       1\n",
      "1 1 1 |    1       1\n"
     ]
    }
   ],
   "source": [
    "print(\"x y z | perceptron expected\")\n",
    "print(\"--------------------------\")\n",
    "\n",
    "for x_val, y_val, z_val in product([0, 1], repeat=3):\n",
    "    # prepare dict: (value, is_positive)\n",
    "    # x and y are positive literals, z is negated\n",
    "    inputs = {\n",
    "        \"x\": (x_val, True),\n",
    "        \"y\": (y_val, True),\n",
    "        \"z\": (z_val, False),\n",
    "    }\n",
    "\n",
    "    percep_out = general_or(inputs)\n",
    "    expected = int((x_val) or (y_val) or not (z_val))  # expected output\n",
    "\n",
    "    print(f\"{x_val} {y_val} {z_val} |    {percep_out}       {expected}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced8523",
   "metadata": {},
   "source": [
    "### To be done later: Implementing the generalized AND/OR, with general distinct weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
