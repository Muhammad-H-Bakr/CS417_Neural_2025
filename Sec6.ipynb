{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe518af",
   "metadata": {},
   "source": [
    "# Multiple Layer Training: Backpropagation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a891d",
   "metadata": {},
   "source": [
    "## Complete Algorithm\n",
    "\n",
    "### Forward Pass\n",
    "```\n",
    "a⁰ = x\n",
    "for l = 1 to L:\n",
    "    zˡ = Wˡ · aˡ⁻¹ + bˡ\n",
    "    aˡ = σ(zˡ)\n",
    "```\n",
    "\n",
    "### Backward Pass\n",
    "```\n",
    "# 1. Compute output error\n",
    "δᴸ = (aᴸ - y) ⊙ aᴸ ⊙ (1 - aᴸ)\n",
    "\n",
    "# 2. Backpropagate through hidden layers\n",
    "for l = L-1 down to 1:\n",
    "    δˡ = (Wˡ⁺¹)ᵀ · δˡ⁺¹ ⊙ aˡ ⊙ (1 - aˡ)\n",
    "\n",
    "# 3. Compute gradients\n",
    "for l = 1 to L:\n",
    "    ∂C/∂Wˡ = δˡ · (aˡ⁻¹)ᵀ\n",
    "    ∂C/∂bˡ = δˡ\n",
    "```\n",
    "\n",
    "### Parameter Update (Gradient Descent)\n",
    "```\n",
    "for l = 1 to L:\n",
    "    Wˡ = Wˡ - η · ∂C/∂Wˡ\n",
    "    bˡ = bˡ - η · ∂C/∂bˡ\n",
    "```\n",
    "Where $\\eta$ is the learning rate.\n",
    "\n",
    "### ====================================\n",
    "### Read from here for your own knowledge; ***you can skip***.\n",
    "### ====================================\n",
    "## Batch Processing (Code Enhancement)\n",
    "\n",
    "Improves the basic algorithm by processing multiple examples simultaneously:\n",
    "\n",
    "- **Input**: $X$ with shape $(input\\_size \\times batch\\_size)$\n",
    "- **Gradients** are averaged over the batch:\n",
    "  $$\\frac{\\partial C}{\\partial W^l} = \\frac{1}{m} \\sum_{i=1}^m \\delta^l_{(i)} (a^{l-1}_{(i)})^T$$\n",
    "  $$\\frac{\\partial C}{\\partial b^l} = \\frac{1}{m} \\sum_{i=1}^m \\delta^l_{(i)}$$\n",
    "\n",
    "## Why This Works: The Chain Rule Insight\n",
    "\n",
    "The fundamental mathematics is the chain rule:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^l} = \\frac{\\partial C}{\\partial a^L} \\cdot \\frac{\\partial a^L}{\\partial a^{L-1}} \\cdot \\ldots \\cdot \\frac{\\partial a^{l+1}}{\\partial a^l} \\cdot \\frac{\\partial a^l}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial W^l}$$\n",
    "\n",
    "Instead of computing this massive chain directly, backpropagation computes it recursively:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z^l} = \\underbrace{\\frac{\\partial C}{\\partial z^{l+1}}}_{\\delta^{l+1}} \\cdot \\underbrace{\\frac{\\partial z^{l+1}}{\\partial a^l}}_{W^{l+1}} \\cdot \\underbrace{\\frac{\\partial a^l}{\\partial z^l}}_{\\sigma'(z^l)}$$\n",
    "\n",
    "## Key Mathematical Insights\n",
    "\n",
    "1. **Local Computation**: Each layer only needs information from adjacent layers\n",
    "2. **Dynamic Programming**: We reuse computed $\\delta^{l+1}$ to compute $\\delta^l$\n",
    "3. **Efficiency**: Storing $z^l$ and $a^l$ during forward pass avoids recomputation\n",
    "4. **Sigmoid Efficiency**: $\\sigma'(z^l) = a^l(1-a^l)$ uses pre-computed activations\n",
    "\n",
    "## The Essence\n",
    "\n",
    "**Backpropagation calculates how much each weight contributes to the final error by:**\n",
    "1. **Forward pass**: Compute all activations\n",
    "2. **Backward pass**: Propagate errors from output back to input\n",
    "3. **Update**: Nudge weights in the direction that reduces error\n",
    "\n",
    "The mathematical elegance is that we can compute exact gradients for millions of parameters using only local information and efficient recursive computation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbf9a9a",
   "metadata": {},
   "source": [
    "## The Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cfcef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "# Mean Squared Error (MSE) Loss\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "# Deep Neural Network Class\n",
    "class DeepNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initialize a deep neural network.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes: List of integers representing the number of neurons in each layer.\n",
    "                         Example: [2, 4, 3, 1] for input=2, hidden1=4, hidden2=3, output=1\n",
    "            learning_rate: Learning rate for gradient descent\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.lr = learning_rate\n",
    "        self.L = len(layer_sizes) - 1  # Number of weight layers\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(self.L):\n",
    "            \"\"\"'He' aka 'Kaiming' initialization for better training of deep networks;\n",
    "            (you can use random initialization and still have it work normally btw).\"\"\"\n",
    "            w = np.random.randn(layer_sizes[i + 1], layer_sizes[i]) * np.sqrt(\n",
    "                2.0 / layer_sizes[i]\n",
    "            )\n",
    "            b = np.zeros((layer_sizes[i + 1], 1))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass through all layers\"\"\"\n",
    "        self.activations = [X]  # Store all activations, starting with input\n",
    "        self.z_values = []  # Store all z values (before activation)\n",
    "\n",
    "        current_activation = X\n",
    "\n",
    "        for i in range(self.L):\n",
    "            z = self.weights[i] @ current_activation + self.biases[i]\n",
    "            current_activation = sigmoid(z)\n",
    "\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(current_activation)\n",
    "\n",
    "        return current_activation\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        m = X.shape[1]\n",
    "        dW = [np.zeros_like(w) for w in self.weights]\n",
    "        db = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Output layer error - using activations for derivative\n",
    "        dZ = (self.activations[-1] - Y) * self.activations[-1] * (1 - self.activations[-1])\n",
    "        dW[-1] = (dZ @ self.activations[-2].T) / m\n",
    "        db[-1] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Backpropagate through hidden layers\n",
    "        for l in range(self.L - 2, -1, -1):\n",
    "            dA = self.weights[l + 1].T @ dZ\n",
    "            dZ = dA * self.activations[l + 1] * (1 - self.activations[l + 1])\n",
    "            dW[l] = (dZ @ self.activations[l].T) / m\n",
    "            db[l] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "\n",
    "        # Update parameters (same as before)\n",
    "        for l in range(self.L):\n",
    "            self.weights[l] -= self.lr * dW[l]\n",
    "            self.biases[l] -= self.lr * db[l]\n",
    "\n",
    "    def train(self, X, Y, epochs=10000, print_every=1000):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            output = self.forward(X)\n",
    "            loss = mse_loss(Y, output)\n",
    "            self.backward(X, Y)\n",
    "\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch} - Loss: {loss:.6f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "    def print_parameters(self):\n",
    "        \"\"\"Print weights and biases in a detailed format\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"DETAILED NETWORK PARAMETERS\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for i in range(self.L):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(\"-\" * 20)\n",
    "            print(f\"Shape: {self.layer_sizes[i]} → {self.layer_sizes[i+1]}\")\n",
    "\n",
    "            print(f\"\\nWeights W{i+1}:\")\n",
    "            print(self.weights[i])\n",
    "\n",
    "            print(f\"\\nBiases b{i+1}:\")\n",
    "            print(self.biases[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306bc178",
   "metadata": {},
   "source": [
    "### Example 1: XNOR (Negated XOR). [2 different architectures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba4c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing Architecture 1: [2, 2, 1]\n",
      "==================================================\n",
      "Network Architecture: [2, 2, 1]\n",
      "Epoch 2500 - Loss: 0.005531\n",
      "Epoch 5000 - Loss: 0.001676\n",
      "Epoch 7500 - Loss: 0.000956\n",
      "Epoch 10000 - Loss: 0.000663\n",
      "\n",
      "Predictions:\n",
      "[[0 1 1 0]]\n",
      "Expected:\n",
      "[[0 1 1 0]]\n",
      "Accuracy: 100.0%\n",
      "\n",
      "==================================================\n",
      "DETAILED NETWORK PARAMETERS\n",
      "==================================================\n",
      "\n",
      "Layer 1:\n",
      "--------------------\n",
      "Shape: 2 → 2\n",
      "\n",
      "Weights W1:\n",
      "[[-4.55115009 -4.55214052]\n",
      " [-6.0840895  -6.0888564 ]]\n",
      "\n",
      "Biases b1:\n",
      "[[6.76396353]\n",
      " [2.42951796]]\n",
      "\n",
      "Layer 2:\n",
      "--------------------\n",
      "Shape: 2 → 1\n",
      "\n",
      "Weights W2:\n",
      "[[ 9.0720307  -9.31196078]]\n",
      "\n",
      "Biases b2:\n",
      "[[-4.25744572]]\n",
      "\n",
      "==================================================\n",
      "Testing Architecture 2: [2, 4, 3, 1]\n",
      "==================================================\n",
      "Network Architecture: [2, 4, 3, 1]\n",
      "Epoch 2500 - Loss: 0.006881\n",
      "Epoch 5000 - Loss: 0.000919\n",
      "Epoch 7500 - Loss: 0.000448\n",
      "Epoch 10000 - Loss: 0.000289\n",
      "\n",
      "Predictions:\n",
      "[[0 1 1 0]]\n",
      "Expected:\n",
      "[[0 1 1 0]]\n",
      "Accuracy: 100.0%\n",
      "\n",
      "==================================================\n",
      "DETAILED NETWORK PARAMETERS\n",
      "==================================================\n",
      "\n",
      "Layer 1:\n",
      "--------------------\n",
      "Shape: 2 → 4\n",
      "\n",
      "Weights W1:\n",
      "[[ 2.08670653 -4.13538263]\n",
      " [ 3.54714705 -0.01671905]\n",
      " [ 5.76475613  5.80337491]\n",
      " [ 1.35298455  1.32866933]]\n",
      "\n",
      "Biases b1:\n",
      "[[-0.80986216]\n",
      " [-1.44588028]\n",
      " [-2.14545754]\n",
      " [-1.73643357]]\n",
      "\n",
      "Layer 2:\n",
      "--------------------\n",
      "Shape: 4 → 3\n",
      "\n",
      "Weights W2:\n",
      "[[-1.72051655  1.24183783 -1.24912299  0.10201314]\n",
      " [-4.06851465  4.37033951 -5.86822835  2.85141637]\n",
      " [-1.61317874  1.97829112 -4.23908973  1.66262497]]\n",
      "\n",
      "Biases b2:\n",
      "[[-0.20912213]\n",
      " [ 1.67974104]\n",
      " [ 1.4423898 ]]\n",
      "\n",
      "Layer 3:\n",
      "--------------------\n",
      "Shape: 3 → 1\n",
      "\n",
      "Weights W3:\n",
      "[[-1.23123063 -8.48455346 -4.71990686]]\n",
      "\n",
      "Biases b3:\n",
      "[[5.90316167]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # XNOR dataset\n",
    "    X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])  # 2 inputs\n",
    "    Y = np.array([[0, 1, 1, 0]])  # 1 output\n",
    "\n",
    "    # Test different architectures\n",
    "    architectures = [\n",
    "        [2, 2, 1],  # Original: 1 hidden layer\n",
    "        [2, 4, 3, 1],  # 2 hidden layers\n",
    "    ]\n",
    "\n",
    "    for i, arch in enumerate(architectures):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing Architecture {i+1}: {arch}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        dnn = DeepNeuralNetwork(layer_sizes=arch, learning_rate=1.0)\n",
    "        print(f\"Network Architecture: {dnn.layer_sizes}\")\n",
    "\n",
    "        dnn.train(X, Y, epochs=10000, print_every=2500)\n",
    "\n",
    "        print(\"\\nPredictions:\")\n",
    "        preds = dnn.predict(X)\n",
    "        print(preds)\n",
    "        print(\"Expected:\")\n",
    "        print(Y)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(preds == Y)\n",
    "        print(f\"Accuracy: {accuracy*100:.1f}%\")\n",
    "\n",
    "        # Print all parameters in detailed format\n",
    "        dnn.print_parameters() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e735fe7",
   "metadata": {},
   "source": [
    "### Example 3: Circular Boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61daeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000 - Loss: 0.000120\n",
      "Epoch 20000 - Loss: 0.000053\n",
      "Epoch 30000 - Loss: 0.000033\n",
      "Epoch 40000 - Loss: 0.000024\n",
      "Epoch 50000 - Loss: 0.000019\n",
      "Epoch 60000 - Loss: 0.000015\n",
      "\n",
      "Input Data (X):\n",
      "[[0.25335268 0.74614652 0.88184622 0.73484997 0.96319048 0.54675465\n",
      "  0.58390522 0.54025938]\n",
      " [0.03328745 0.38489802 0.49099051 0.9442814  0.68576348 0.49448593\n",
      "  0.52153613 0.03154741]]\n",
      "\n",
      "Expected Output (Y):\n",
      "[[0 1 1 1 1 1 1 0]]\n",
      "\n",
      "Predictions:\n",
      "[[0 1 1 1 1 1 1 0]]\n",
      "\n",
      "==================================================\n",
      "DETAILED NETWORK PARAMETERS\n",
      "==================================================\n",
      "\n",
      "Layer 1:\n",
      "--------------------\n",
      "Shape: 2 → 8\n",
      "\n",
      "Weights W1:\n",
      "[[ 1.44973103  3.9747974 ]\n",
      " [ 1.4490117   1.83976245]\n",
      " [-0.74202255 -0.53225695]\n",
      " [ 0.23454735 -0.9068718 ]\n",
      " [ 0.49963383  2.36529899]\n",
      " [-0.81507762 -4.74976815]\n",
      " [-0.49520996 -1.04680001]\n",
      " [-1.58550405 -1.39571535]]\n",
      "\n",
      "Biases b1:\n",
      "[[-1.6275902 ]\n",
      " [-0.91345002]\n",
      " [ 0.10956359]\n",
      " [ 0.00614543]\n",
      " [-0.55488528]\n",
      " [ 1.44143708]\n",
      " [ 0.1397682 ]\n",
      " [ 0.66556763]]\n",
      "\n",
      "Layer 2:\n",
      "--------------------\n",
      "Shape: 8 → 4\n",
      "\n",
      "Weights W2:\n",
      "[[-1.21503017 -1.15414077  0.05533435  0.42504381 -0.86091127  1.84479751\n",
      "   0.56798887  0.9497004 ]\n",
      " [ 3.40004773  1.90212567 -0.68348776 -0.14286716  1.69099851 -4.18423453\n",
      "  -0.82067868 -1.42892447]\n",
      " [ 0.49228855  0.5805291  -0.63032244 -0.18617006  0.40291371 -0.24132721\n",
      "  -0.20835032 -0.42255415]\n",
      " [-2.56035469 -0.85966824  0.78073038  1.52578612 -1.37362806  1.87307113\n",
      "   0.82869209  0.90588151]]\n",
      "\n",
      "Biases b2:\n",
      "[[ 0.14072057]\n",
      " [-0.73896102]\n",
      " [-0.06249817]\n",
      " [ 0.05084399]]\n",
      "\n",
      "Layer 3:\n",
      "--------------------\n",
      "Shape: 4 → 1\n",
      "\n",
      "Weights W3:\n",
      "[[-3.10000773  7.47256532  0.86364641 -4.12933367]]\n",
      "\n",
      "Biases b3:\n",
      "[[-0.39478097]]\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(0)\n",
    "n = 8  # keep small for display\n",
    "X = np.random.rand(2, n)\n",
    "Y = np.array([(x1**2 + x2**2 > 0.5).astype(int) for x1, x2 in X.T]).reshape(1, -1)\n",
    "\n",
    "nn = DeepNeuralNetwork([2, 8, 4, 1], learning_rate=1.0)\n",
    "nn.train(X, Y, epochs=60000, print_every=10000)\n",
    "\n",
    "print(\"\\nInput Data (X):\")\n",
    "print(X)\n",
    "print(\"\\nExpected Output (Y):\")\n",
    "print(Y)\n",
    "print(\"\\nPredictions:\")\n",
    "print(nn.predict(X))\n",
    "nn.print_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b9b568",
   "metadata": {},
   "source": [
    "### Example 4: Sanity Checks [OR/AND]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e928431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000 - Loss: 0.011472\n",
      "Epoch 4000 - Loss: 0.005178\n",
      "Epoch 6000 - Loss: 0.003268\n",
      "Epoch 8000 - Loss: 0.002368\n",
      "Epoch 10000 - Loss: 0.001849\n",
      "\n",
      "Input Data (X):\n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "\n",
      "Expected Output (Y):\n",
      "[[0 1 1 1]]\n",
      "\n",
      "Predictions:\n",
      "[[0 1 1 1]]\n",
      "\n",
      "==================================================\n",
      "DETAILED NETWORK PARAMETERS\n",
      "==================================================\n",
      "\n",
      "Layer 1:\n",
      "--------------------\n",
      "Shape: 2 → 1\n",
      "\n",
      "Weights W1:\n",
      "[[5.84650411 5.8473781 ]]\n",
      "\n",
      "Biases b1:\n",
      "[[-2.6741001]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 1]]) # OR operation\n",
    "\n",
    "nn = DeepNeuralNetwork([2, 1], learning_rate=0.3)\n",
    "nn.train(X, Y, epochs=10000, print_every=2000)\n",
    "\n",
    "print(\"\\nInput Data (X):\")\n",
    "print(X)\n",
    "print(\"\\nExpected Output (Y):\")\n",
    "print(Y)\n",
    "print(\"\\nPredictions:\")\n",
    "print(nn.predict(X))\n",
    "nn.print_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1009d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 - Loss: 0.043621\n",
      "Epoch 2000 - Loss: 0.021935\n",
      "Epoch 3000 - Loss: 0.014105\n",
      "Epoch 4000 - Loss: 0.010227\n",
      "Epoch 5000 - Loss: 0.007955\n",
      "\n",
      "Input Data (X):\n",
      "[[0 0 1 1]\n",
      " [0 1 0 1]]\n",
      "\n",
      "Expected Output (Y):\n",
      "[[0 0 0 1]]\n",
      "\n",
      "Predictions:\n",
      "[[0 0 0 1]]\n",
      "\n",
      "==================================================\n",
      "DETAILED NETWORK PARAMETERS\n",
      "==================================================\n",
      "\n",
      "Layer 1:\n",
      "--------------------\n",
      "Shape: 2 → 1\n",
      "\n",
      "Weights W1:\n",
      "[[4.27796714 4.27796683]]\n",
      "\n",
      "Biases b1:\n",
      "[[-6.51719745]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0, 1, 1], [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 0, 0, 1]]) # AND operation\n",
    "\n",
    "nn = DeepNeuralNetwork([2, 1], learning_rate=0.3)\n",
    "nn.train(X, Y, epochs=5000, print_every=1000)\n",
    "\n",
    "print(\"\\nInput Data (X):\")\n",
    "print(X)\n",
    "print(\"\\nExpected Output (Y):\")\n",
    "print(Y)\n",
    "print(\"\\nPredictions:\")\n",
    "print(nn.predict(X))\n",
    "nn.print_parameters()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
